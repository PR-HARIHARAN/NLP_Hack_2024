{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill the Null values with unknown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crimeaditionalinfo'].fillna('',inplace=True)\n",
    "df['sub_category'].fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing all special character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['crimeaditionalinfo'] = df['crimeaditionalinfo'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lower-casing all the sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Cyber Bullying  Stalking  Sexting</td>\n",
       "      <td>i had continue received random calls and abusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>the above fraudster is continuously messaging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>he is acting like a police and demanding for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Job Fraud</td>\n",
       "      <td>in apna job i have applied for job interview f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>i received a call from lady stating that she w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124910</th>\n",
       "      <td>124910</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Matrimonial Fraud</td>\n",
       "      <td>a lady named rashmi probably a fake name had c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124911</th>\n",
       "      <td>124911</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Internet Banking Related Fraud</td>\n",
       "      <td>i am mr chokhe ram  two pers mobile number wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124912</th>\n",
       "      <td>124912</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td>mai bibekbraj maine pahle ki complain kar chuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124913</th>\n",
       "      <td>124913</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Internet Banking Related Fraud</td>\n",
       "      <td>received url link for updating kyc from mobile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124914</th>\n",
       "      <td>124914</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td>i saw add on facebook for job placement and i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124915 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                               category  \\\n",
       "0                0  Online and Social Media Related Crime   \n",
       "1                1                 Online Financial Fraud   \n",
       "2                2               Online Gambling  Betting   \n",
       "3                3  Online and Social Media Related Crime   \n",
       "4                4                 Online Financial Fraud   \n",
       "...            ...                                    ...   \n",
       "124910      124910  Online and Social Media Related Crime   \n",
       "124911      124911                 Online Financial Fraud   \n",
       "124912      124912                  Any Other Cyber Crime   \n",
       "124913      124913                 Online Financial Fraud   \n",
       "124914      124914                  Any Other Cyber Crime   \n",
       "\n",
       "                             sub_category  \\\n",
       "0       Cyber Bullying  Stalking  Sexting   \n",
       "1                       Fraud CallVishing   \n",
       "2                Online Gambling  Betting   \n",
       "3                        Online Job Fraud   \n",
       "4                       Fraud CallVishing   \n",
       "...                                   ...   \n",
       "124910           Online Matrimonial Fraud   \n",
       "124911     Internet Banking Related Fraud   \n",
       "124912                              Other   \n",
       "124913     Internet Banking Related Fraud   \n",
       "124914                              Other   \n",
       "\n",
       "                                       crimeaditionalinfo  \n",
       "0       i had continue received random calls and abusi...  \n",
       "1       the above fraudster is continuously messaging ...  \n",
       "2       he is acting like a police and demanding for m...  \n",
       "3       in apna job i have applied for job interview f...  \n",
       "4       i received a call from lady stating that she w...  \n",
       "...                                                   ...  \n",
       "124910  a lady named rashmi probably a fake name had c...  \n",
       "124911  i am mr chokhe ram  two pers mobile number wer...  \n",
       "124912  mai bibekbraj maine pahle ki complain kar chuk...  \n",
       "124913  received url link for updating kyc from mobile...  \n",
       "124914  i saw add on facebook for job placement and i ...  \n",
       "\n",
       "[124915 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['crimeaditionalinfo']= df['crimeaditionalinfo'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom stop word removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([\n",
      "    'a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', 'ain't', 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', 'aren't', 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', 'can't', 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', 'c'mon', 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', 'couldn't', 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', 'didn't', 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', 'doesn't', 'doing', 'done', 'dono', 'dont', 'don't', 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', 'hadn't', 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', 'hasn't', 'have', 'haven', 'havent', 'haven't', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'here's', 'hereupon', 'hers', 'herself', 'he's', 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', 'i'd', 'idk', 'ie', 'if', 'i'll', 'i'm', 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', 'isn't', 'iss', 'isse', 'issi', 'isski', 'it', 'it'd', 'it'll', 'itna', 'itne', 'itni', 'itno', 'its', 'it's', 'itself', 'ityaadi', 'ityadi', 'i've', 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', 'let's', 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', 'mightn't', 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', 'mustn't', 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', 'needn't', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', 'shan't', 'she', 'she's', 'should', 'shouldn', 'shouldnt', 'shouldn't', 'should've', 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', 'that'll', 'thats', 'that's', 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'there's', 'thereupon', 'these', 'they', 'they'd', 'they'll', 'they're', 'they've', 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', 'wasn't', 'way', 'we', 'we'd', 'well', 'we'll', 'went', 'were', 'we're', 'weren', 'werent', 'weren't', 'we've', 'what', 'whatever', 'what's', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'where's', 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', 'who's', 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', 'won't', 'would', 'wouldn', 'wouldnt', 'wouldn't', 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', 'you'd', 'you'll', 'your', 'you're', 'yours', 'yourself', 'yourselves', 'you've', 'yup'\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## the data also contains Hinglish,other indain languages\n",
    "#source : https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish\n",
    "\n",
    "\n",
    "# Read the words from the file and convert them into the required format\n",
    "\n",
    "file_path = 'stop_hinglish.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = [line.strip() for line in file if line.strip()]\n",
    "formatted_set = \"set([\\n    '\" + \"', '\".join(words) + \"'\\n])\"\n",
    "print(formatted_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng = set(stopwords.words('english'))\n",
    "hin = set(stopwords.words('hinglish'))\n",
    "\n",
    "combined = eng.union(hin).union(formatted_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x: [word for word in x if word not in combined])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Cyber Bullying  Stalking  Sexting</td>\n",
       "      <td>[continu, receiv, random, call, abus, messag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>[fraudster, continu, messag, pay, money, send,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>[act, polic, demand, money, ad, section, text,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Job Fraud</td>\n",
       "      <td>[job, appli, job, interview, telecal, resourc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>[receiv, call, ladi, state, send, phone, vivo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124910</th>\n",
       "      <td>124910</td>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Matrimonial Fraud</td>\n",
       "      <td>[ladi, name, rashmi, probabl, fake, call, day,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124911</th>\n",
       "      <td>124911</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Internet Banking Related Fraud</td>\n",
       "      <td>[mr, chokh, ram, per, mobil, number, found, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124912</th>\n",
       "      <td>124912</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td>[bibekbraj, pahl, complain, chuka, financi, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124913</th>\n",
       "      <td>124913</td>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Internet Banking Related Fraud</td>\n",
       "      <td>[receiv, url, link, updat, kyc, mobil, open, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124914</th>\n",
       "      <td>124914</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td>[add, facebook, job, placement, job, contact, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124915 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                               category  \\\n",
       "0                0  Online and Social Media Related Crime   \n",
       "1                1                 Online Financial Fraud   \n",
       "2                2               Online Gambling  Betting   \n",
       "3                3  Online and Social Media Related Crime   \n",
       "4                4                 Online Financial Fraud   \n",
       "...            ...                                    ...   \n",
       "124910      124910  Online and Social Media Related Crime   \n",
       "124911      124911                 Online Financial Fraud   \n",
       "124912      124912                  Any Other Cyber Crime   \n",
       "124913      124913                 Online Financial Fraud   \n",
       "124914      124914                  Any Other Cyber Crime   \n",
       "\n",
       "                             sub_category  \\\n",
       "0       Cyber Bullying  Stalking  Sexting   \n",
       "1                       Fraud CallVishing   \n",
       "2                Online Gambling  Betting   \n",
       "3                        Online Job Fraud   \n",
       "4                       Fraud CallVishing   \n",
       "...                                   ...   \n",
       "124910           Online Matrimonial Fraud   \n",
       "124911     Internet Banking Related Fraud   \n",
       "124912                              Other   \n",
       "124913     Internet Banking Related Fraud   \n",
       "124914                              Other   \n",
       "\n",
       "                                       crimeaditionalinfo  \n",
       "0       [continu, receiv, random, call, abus, messag, ...  \n",
       "1       [fraudster, continu, messag, pay, money, send,...  \n",
       "2       [act, polic, demand, money, ad, section, text,...  \n",
       "3       [job, appli, job, interview, telecal, resourc,...  \n",
       "4       [receiv, call, ladi, state, send, phone, vivo,...  \n",
       "...                                                   ...  \n",
       "124910  [ladi, name, rashmi, probabl, fake, call, day,...  \n",
       "124911  [mr, chokh, ram, per, mobil, number, found, go...  \n",
       "124912  [bibekbraj, pahl, complain, chuka, financi, fr...  \n",
       "124913  [receiv, url, link, updat, kyc, mobil, open, r...  \n",
       "124914  [add, facebook, job, placement, job, contact, ...  \n",
       "\n",
       "[124915 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x:[lemmatizer.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens to string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x: ' '.join(x))\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['crimeaditionalinfo']=df['crimeaditionalinfo'].apply(lambda x: x.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting the \"crimeaditionalinfo\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>803</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>856</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1282</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1544</td>\n",
       "      <td>Any Other Cyber Crime</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124910</th>\n",
       "      <td>83811</td>\n",
       "      <td>Sexually Obscene material</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yesterday pm girl account komal roy call faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124911</th>\n",
       "      <td>80842</td>\n",
       "      <td>Sexually Obscene material</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yesterday rd march whatsapp receiv edit sexual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124912</th>\n",
       "      <td>122691</td>\n",
       "      <td>Sexually Obscene material</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yesterday whatsapp video call watch min person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124913</th>\n",
       "      <td>109017</td>\n",
       "      <td>Sexually Obscene material</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yhe call kr pereshan phele telegram kra asa pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124914</th>\n",
       "      <td>116780</td>\n",
       "      <td>Sexually Obscene material</td>\n",
       "      <td>unknown</td>\n",
       "      <td>zest money farud haa zest money call gya regis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124915 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                   category sub_category  \\\n",
       "0              803      Any Other Cyber Crime        Other   \n",
       "1              856      Any Other Cyber Crime        Other   \n",
       "2             1282      Any Other Cyber Crime        Other   \n",
       "3             1300      Any Other Cyber Crime        Other   \n",
       "4             1544      Any Other Cyber Crime        Other   \n",
       "...            ...                        ...          ...   \n",
       "124910       83811  Sexually Obscene material      unknown   \n",
       "124911       80842  Sexually Obscene material      unknown   \n",
       "124912      122691  Sexually Obscene material      unknown   \n",
       "124913      109017  Sexually Obscene material      unknown   \n",
       "124914      116780  Sexually Obscene material      unknown   \n",
       "\n",
       "                                       crimeaditionalinfo  \n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                   ...  \n",
       "124910  yesterday pm girl account komal roy call faceb...  \n",
       "124911  yesterday rd march whatsapp receiv edit sexual...  \n",
       "124912  yesterday whatsapp video call watch min person...  \n",
       "124913  yhe call kr pereshan phele telegram kra asa pl...  \n",
       "124914  zest money farud haa zest money call gya regis...  \n",
       "\n",
       "[124915 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "group = df.groupby(['category','sub_category'])\n",
    "sort = df.sort_values(by=['category','sub_category','crimeaditionalinfo']).reset_index(drop=True)\n",
    "sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124915 entries, 0 to 124914\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Unnamed: 0          124915 non-null  int64 \n",
      " 1   category            124915 non-null  object\n",
      " 2   sub_category        124915 non-null  object\n",
      " 3   crimeaditionalinfo  124915 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "sorted_df = sort.dropna()\n",
    "sorted_df.info()\n",
    "data = sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, GRU, Dense, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing or non-string values in 'crimeaditionalinfo' column\n",
    "data['crimeaditionalinfo'] = data['crimeaditionalinfo'].fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'category' and 'sub_category'\n",
    "category_encoder = LabelEncoder()\n",
    "sub_category_encoder = LabelEncoder()\n",
    "data['category'] = category_encoder.fit_transform(data['category'])\n",
    "data['sub_category'] = sub_category_encoder.fit_transform(data['sub_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(data['crimeaditionalinfo'])\n",
    "X = tokenizer.texts_to_sequences(data['crimeaditionalinfo'])\n",
    "X = pad_sequences(X, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode targets\n",
    "num_category_classes = len(category_encoder.classes_)\n",
    "num_sub_category_classes = len(sub_category_encoder.classes_)\n",
    "y_category = np.eye(num_category_classes)[data['category']]\n",
    "y_sub_category = np.eye(num_sub_category_classes)[data['sub_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_category_train, y_category_test, y_sub_category_train, y_sub_category_test = train_test_split(\n",
    "    X, y_category, y_sub_category, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing components saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessing components\n",
    "with open(\"tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "with open(\"category_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(category_encoder, file)\n",
    "\n",
    "with open(\"sub_category_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(sub_category_encoder, file)\n",
    "\n",
    "print(\"Preprocessing components saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model builder\n",
    "def build_multi_output_model(model_type):\n",
    "    input_layer = Input(shape=(100,))\n",
    "    embedding = Embedding(input_dim=10000, output_dim=128, input_length=100)(input_layer)\n",
    "    \n",
    "    if model_type == 'SimpleRNN':\n",
    "        x = SimpleRNN(64)(embedding)\n",
    "    elif model_type == 'LSTM':\n",
    "        x = LSTM(64)(embedding)\n",
    "    elif model_type == 'GRU':\n",
    "        x = GRU(64)(embedding)\n",
    "    elif model_type == 'Bi-LSTM':\n",
    "        x = Bidirectional(LSTM(64))(embedding)\n",
    "    \n",
    "    category_output = Dense(num_category_classes, activation='softmax', name='category_output')(x)\n",
    "    sub_category_output = Dense(num_sub_category_classes, activation='softmax', name='sub_category_output')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=[category_output, sub_category_output])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'category_output': 'categorical_crossentropy',\n",
    "            'sub_category_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        metrics={\n",
    "            'category_output': ['accuracy'],\n",
    "            'sub_category_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleRNN model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP_cyber\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - category_output_accuracy: 0.7053 - category_output_loss: 0.9820 - loss: 3.1007 - sub_category_output_accuracy: 0.3586 - sub_category_output_loss: 2.1187 - val_category_output_accuracy: 0.7344 - val_category_output_loss: 0.7916 - val_loss: 2.5188 - val_sub_category_output_accuracy: 0.4421 - val_sub_category_output_loss: 1.7272\n",
      "Epoch 2/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13ms/step - category_output_accuracy: 0.7565 - category_output_loss: 0.7291 - loss: 2.3380 - sub_category_output_accuracy: 0.4933 - sub_category_output_loss: 1.6089 - val_category_output_accuracy: 0.7273 - val_category_output_loss: 0.8035 - val_loss: 2.4724 - val_sub_category_output_accuracy: 0.4860 - val_sub_category_output_loss: 1.6690\n",
      "Epoch 3/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - category_output_accuracy: 0.7699 - category_output_loss: 0.6999 - loss: 2.2397 - sub_category_output_accuracy: 0.5230 - sub_category_output_loss: 1.5398 - val_category_output_accuracy: 0.7344 - val_category_output_loss: 0.8042 - val_loss: 2.4454 - val_sub_category_output_accuracy: 0.4941 - val_sub_category_output_loss: 1.6413\n",
      "Epoch 4/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 15ms/step - category_output_accuracy: 0.7945 - category_output_loss: 0.6216 - loss: 2.0387 - sub_category_output_accuracy: 0.5594 - sub_category_output_loss: 1.4171 - val_category_output_accuracy: 0.7259 - val_category_output_loss: 0.8551 - val_loss: 2.6275 - val_sub_category_output_accuracy: 0.4570 - val_sub_category_output_loss: 1.7724\n",
      "Epoch 5/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 16ms/step - category_output_accuracy: 0.8138 - category_output_loss: 0.5608 - loss: 1.8555 - sub_category_output_accuracy: 0.5984 - sub_category_output_loss: 1.2947 - val_category_output_accuracy: 0.7246 - val_category_output_loss: 0.8660 - val_loss: 2.5875 - val_sub_category_output_accuracy: 0.4921 - val_sub_category_output_loss: 1.7215\n",
      "Epoch 6/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 16ms/step - category_output_accuracy: 0.8328 - category_output_loss: 0.5093 - loss: 1.7060 - sub_category_output_accuracy: 0.6296 - sub_category_output_loss: 1.1967 - val_category_output_accuracy: 0.7139 - val_category_output_loss: 0.9200 - val_loss: 2.7398 - val_sub_category_output_accuracy: 0.4690 - val_sub_category_output_loss: 1.8198\n",
      "Epoch 7/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - category_output_accuracy: 0.8500 - category_output_loss: 0.4605 - loss: 1.5912 - sub_category_output_accuracy: 0.6538 - sub_category_output_loss: 1.1307 - val_category_output_accuracy: 0.7070 - val_category_output_loss: 0.9627 - val_loss: 2.8104 - val_sub_category_output_accuracy: 0.4679 - val_sub_category_output_loss: 1.8477\n",
      "Epoch 8/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - category_output_accuracy: 0.8722 - category_output_loss: 0.3999 - loss: 1.4343 - sub_category_output_accuracy: 0.6808 - sub_category_output_loss: 1.0344 - val_category_output_accuracy: 0.7014 - val_category_output_loss: 1.0171 - val_loss: 2.9769 - val_sub_category_output_accuracy: 0.4351 - val_sub_category_output_loss: 1.9598\n",
      "Epoch 9/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - category_output_accuracy: 0.8783 - category_output_loss: 0.3848 - loss: 1.4304 - sub_category_output_accuracy: 0.6691 - sub_category_output_loss: 1.0456 - val_category_output_accuracy: 0.6708 - val_category_output_loss: 1.1410 - val_loss: 3.2192 - val_sub_category_output_accuracy: 0.4496 - val_sub_category_output_loss: 2.0782\n",
      "Epoch 10/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - category_output_accuracy: 0.8870 - category_output_loss: 0.3530 - loss: 1.3115 - sub_category_output_accuracy: 0.7039 - sub_category_output_loss: 0.9585 - val_category_output_accuracy: 0.7059 - val_category_output_loss: 1.1287 - val_loss: 3.1728 - val_sub_category_output_accuracy: 0.4747 - val_sub_category_output_loss: 2.0441\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP_cyber\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 24ms/step - category_output_accuracy: 0.7166 - category_output_loss: 0.9536 - loss: 2.9439 - sub_category_output_accuracy: 0.3855 - sub_category_output_loss: 1.9903 - val_category_output_accuracy: 0.7480 - val_category_output_loss: 0.7349 - val_loss: 2.2371 - val_sub_category_output_accuracy: 0.5291 - val_sub_category_output_loss: 1.5022\n",
      "Epoch 2/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 41ms/step - category_output_accuracy: 0.7648 - category_output_loss: 0.6827 - loss: 2.1139 - sub_category_output_accuracy: 0.5504 - sub_category_output_loss: 1.4312 - val_category_output_accuracy: 0.7557 - val_category_output_loss: 0.7104 - val_loss: 2.1466 - val_sub_category_output_accuracy: 0.5480 - val_sub_category_output_loss: 1.4363\n",
      "Epoch 3/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 30ms/step - category_output_accuracy: 0.7904 - category_output_loss: 0.6079 - loss: 1.8922 - sub_category_output_accuracy: 0.5945 - sub_category_output_loss: 1.2843 - val_category_output_accuracy: 0.7502 - val_category_output_loss: 0.7158 - val_loss: 2.1477 - val_sub_category_output_accuracy: 0.5498 - val_sub_category_output_loss: 1.4320\n",
      "Epoch 4/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 40ms/step - category_output_accuracy: 0.8120 - category_output_loss: 0.5447 - loss: 1.7223 - sub_category_output_accuracy: 0.6293 - sub_category_output_loss: 1.1777 - val_category_output_accuracy: 0.7392 - val_category_output_loss: 0.7494 - val_loss: 2.2252 - val_sub_category_output_accuracy: 0.5421 - val_sub_category_output_loss: 1.4758\n",
      "Epoch 5/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 41ms/step - category_output_accuracy: 0.8378 - category_output_loss: 0.4829 - loss: 1.5600 - sub_category_output_accuracy: 0.6605 - sub_category_output_loss: 1.0771 - val_category_output_accuracy: 0.7367 - val_category_output_loss: 0.7903 - val_loss: 2.3317 - val_sub_category_output_accuracy: 0.5320 - val_sub_category_output_loss: 1.5415\n",
      "Epoch 6/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 40ms/step - category_output_accuracy: 0.8575 - category_output_loss: 0.4257 - loss: 1.4017 - sub_category_output_accuracy: 0.6907 - sub_category_output_loss: 0.9761 - val_category_output_accuracy: 0.7350 - val_category_output_loss: 0.8623 - val_loss: 2.4938 - val_sub_category_output_accuracy: 0.5292 - val_sub_category_output_loss: 1.6316\n",
      "Epoch 7/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 38ms/step - category_output_accuracy: 0.8776 - category_output_loss: 0.3694 - loss: 1.2505 - sub_category_output_accuracy: 0.7240 - sub_category_output_loss: 0.8811 - val_category_output_accuracy: 0.7357 - val_category_output_loss: 0.9261 - val_loss: 2.6437 - val_sub_category_output_accuracy: 0.5204 - val_sub_category_output_loss: 1.7177\n",
      "Epoch 8/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 29ms/step - category_output_accuracy: 0.8943 - category_output_loss: 0.3258 - loss: 1.1341 - sub_category_output_accuracy: 0.7460 - sub_category_output_loss: 0.8083 - val_category_output_accuracy: 0.7216 - val_category_output_loss: 1.0259 - val_loss: 2.8695 - val_sub_category_output_accuracy: 0.5165 - val_sub_category_output_loss: 1.8437\n",
      "Epoch 9/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 25ms/step - category_output_accuracy: 0.9083 - category_output_loss: 0.2817 - loss: 1.0138 - sub_category_output_accuracy: 0.7663 - sub_category_output_loss: 0.7320 - val_category_output_accuracy: 0.7232 - val_category_output_loss: 1.1009 - val_loss: 3.0270 - val_sub_category_output_accuracy: 0.5080 - val_sub_category_output_loss: 1.9262\n",
      "Epoch 10/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 39ms/step - category_output_accuracy: 0.9209 - category_output_loss: 0.2471 - loss: 0.9190 - sub_category_output_accuracy: 0.7879 - sub_category_output_loss: 0.6719 - val_category_output_accuracy: 0.7127 - val_category_output_loss: 1.2198 - val_loss: 3.2952 - val_sub_category_output_accuracy: 0.5033 - val_sub_category_output_loss: 2.0755\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP_cyber\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 35ms/step - category_output_accuracy: 0.7126 - category_output_loss: 0.9594 - loss: 2.9582 - sub_category_output_accuracy: 0.3965 - sub_category_output_loss: 1.9988 - val_category_output_accuracy: 0.7542 - val_category_output_loss: 0.7121 - val_loss: 2.1748 - val_sub_category_output_accuracy: 0.5452 - val_sub_category_output_loss: 1.4628\n",
      "Epoch 2/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 34ms/step - category_output_accuracy: 0.7761 - category_output_loss: 0.6511 - loss: 2.0225 - sub_category_output_accuracy: 0.5695 - sub_category_output_loss: 1.3714 - val_category_output_accuracy: 0.7524 - val_category_output_loss: 0.7086 - val_loss: 2.1295 - val_sub_category_output_accuracy: 0.5475 - val_sub_category_output_loss: 1.4210\n",
      "Epoch 3/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 17ms/step - category_output_accuracy: 0.8035 - category_output_loss: 0.5690 - loss: 1.7995 - sub_category_output_accuracy: 0.6095 - sub_category_output_loss: 1.2305 - val_category_output_accuracy: 0.7509 - val_category_output_loss: 0.7204 - val_loss: 2.1517 - val_sub_category_output_accuracy: 0.5502 - val_sub_category_output_loss: 1.4314\n",
      "Epoch 4/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 34ms/step - category_output_accuracy: 0.8228 - category_output_loss: 0.5142 - loss: 1.6324 - sub_category_output_accuracy: 0.6455 - sub_category_output_loss: 1.1183 - val_category_output_accuracy: 0.7447 - val_category_output_loss: 0.7693 - val_loss: 2.2784 - val_sub_category_output_accuracy: 0.5360 - val_sub_category_output_loss: 1.5093\n",
      "Epoch 5/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 35ms/step - category_output_accuracy: 0.8506 - category_output_loss: 0.4442 - loss: 1.4429 - sub_category_output_accuracy: 0.6844 - sub_category_output_loss: 0.9987 - val_category_output_accuracy: 0.7394 - val_category_output_loss: 0.8506 - val_loss: 2.4643 - val_sub_category_output_accuracy: 0.5290 - val_sub_category_output_loss: 1.6138\n",
      "Epoch 6/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 35ms/step - category_output_accuracy: 0.8733 - category_output_loss: 0.3799 - loss: 1.2689 - sub_category_output_accuracy: 0.7196 - sub_category_output_loss: 0.8890 - val_category_output_accuracy: 0.7244 - val_category_output_loss: 0.9343 - val_loss: 2.6767 - val_sub_category_output_accuracy: 0.5124 - val_sub_category_output_loss: 1.7425\n",
      "Epoch 7/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 35ms/step - category_output_accuracy: 0.8960 - category_output_loss: 0.3203 - loss: 1.1038 - sub_category_output_accuracy: 0.7545 - sub_category_output_loss: 0.7835 - val_category_output_accuracy: 0.7138 - val_category_output_loss: 1.0419 - val_loss: 2.9292 - val_sub_category_output_accuracy: 0.5062 - val_sub_category_output_loss: 1.8875\n",
      "Epoch 8/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 34ms/step - category_output_accuracy: 0.9146 - category_output_loss: 0.2657 - loss: 0.9576 - sub_category_output_accuracy: 0.7803 - sub_category_output_loss: 0.6919 - val_category_output_accuracy: 0.7188 - val_category_output_loss: 1.1507 - val_loss: 3.1892 - val_sub_category_output_accuracy: 0.5006 - val_sub_category_output_loss: 2.0385\n",
      "Epoch 9/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 24ms/step - category_output_accuracy: 0.9275 - category_output_loss: 0.2250 - loss: 0.8428 - sub_category_output_accuracy: 0.8057 - sub_category_output_loss: 0.6179 - val_category_output_accuracy: 0.7072 - val_category_output_loss: 1.2797 - val_loss: 3.4849 - val_sub_category_output_accuracy: 0.4921 - val_sub_category_output_loss: 2.2053\n",
      "Epoch 10/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 35ms/step - category_output_accuracy: 0.9415 - category_output_loss: 0.1888 - loss: 0.7433 - sub_category_output_accuracy: 0.8251 - sub_category_output_loss: 0.5545 - val_category_output_accuracy: 0.7083 - val_category_output_loss: 1.4271 - val_loss: 3.8131 - val_sub_category_output_accuracy: 0.4904 - val_sub_category_output_loss: 2.3862\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bi-LSTM model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP_cyber\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 31ms/step - category_output_accuracy: 0.7213 - category_output_loss: 0.9061 - loss: 2.8289 - sub_category_output_accuracy: 0.4067 - sub_category_output_loss: 1.9228 - val_category_output_accuracy: 0.7478 - val_category_output_loss: 0.7269 - val_loss: 2.2220 - val_sub_category_output_accuracy: 0.5310 - val_sub_category_output_loss: 1.4951\n",
      "Epoch 2/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 31ms/step - category_output_accuracy: 0.7663 - category_output_loss: 0.6722 - loss: 2.0851 - sub_category_output_accuracy: 0.5536 - sub_category_output_loss: 1.4129 - val_category_output_accuracy: 0.7555 - val_category_output_loss: 0.7009 - val_loss: 2.1386 - val_sub_category_output_accuracy: 0.5477 - val_sub_category_output_loss: 1.4378\n",
      "Epoch 3/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 31ms/step - category_output_accuracy: 0.7930 - category_output_loss: 0.5974 - loss: 1.8749 - sub_category_output_accuracy: 0.5987 - sub_category_output_loss: 1.2775 - val_category_output_accuracy: 0.7523 - val_category_output_loss: 0.7131 - val_loss: 2.1463 - val_sub_category_output_accuracy: 0.5536 - val_sub_category_output_loss: 1.4333\n",
      "Epoch 4/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 27ms/step - category_output_accuracy: 0.8165 - category_output_loss: 0.5377 - loss: 1.7079 - sub_category_output_accuracy: 0.6322 - sub_category_output_loss: 1.1702 - val_category_output_accuracy: 0.7487 - val_category_output_loss: 0.7439 - val_loss: 2.2153 - val_sub_category_output_accuracy: 0.5452 - val_sub_category_output_loss: 1.4715\n",
      "Epoch 5/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 23ms/step - category_output_accuracy: 0.8417 - category_output_loss: 0.4713 - loss: 1.5288 - sub_category_output_accuracy: 0.6675 - sub_category_output_loss: 1.0575 - val_category_output_accuracy: 0.7439 - val_category_output_loss: 0.7949 - val_loss: 2.3375 - val_sub_category_output_accuracy: 0.5376 - val_sub_category_output_loss: 1.5428\n",
      "Epoch 6/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 30ms/step - category_output_accuracy: 0.8624 - category_output_loss: 0.4153 - loss: 1.3795 - sub_category_output_accuracy: 0.6946 - sub_category_output_loss: 0.9642 - val_category_output_accuracy: 0.7395 - val_category_output_loss: 0.8735 - val_loss: 2.5162 - val_sub_category_output_accuracy: 0.5367 - val_sub_category_output_loss: 1.6428\n",
      "Epoch 7/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 29ms/step - category_output_accuracy: 0.8843 - category_output_loss: 0.3564 - loss: 1.2263 - sub_category_output_accuracy: 0.7248 - sub_category_output_loss: 0.8699 - val_category_output_accuracy: 0.7330 - val_category_output_loss: 0.9382 - val_loss: 2.6725 - val_sub_category_output_accuracy: 0.5214 - val_sub_category_output_loss: 1.7345\n",
      "Epoch 8/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 29ms/step - category_output_accuracy: 0.8991 - category_output_loss: 0.3079 - loss: 1.0944 - sub_category_output_accuracy: 0.7490 - sub_category_output_loss: 0.7865 - val_category_output_accuracy: 0.7229 - val_category_output_loss: 1.0468 - val_loss: 2.9111 - val_sub_category_output_accuracy: 0.5164 - val_sub_category_output_loss: 1.8645\n",
      "Epoch 9/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 22ms/step - category_output_accuracy: 0.9129 - category_output_loss: 0.2727 - loss: 0.9969 - sub_category_output_accuracy: 0.7687 - sub_category_output_loss: 0.7243 - val_category_output_accuracy: 0.7262 - val_category_output_loss: 1.1447 - val_loss: 3.1233 - val_sub_category_output_accuracy: 0.5136 - val_sub_category_output_loss: 1.9787\n",
      "Epoch 10/10\n",
      "\u001b[1m2186/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 20ms/step - category_output_accuracy: 0.9254 - category_output_loss: 0.2316 - loss: 0.8826 - sub_category_output_accuracy: 0.7905 - sub_category_output_loss: 0.6510 - val_category_output_accuracy: 0.7171 - val_category_output_loss: 1.2982 - val_loss: 3.4646 - val_sub_category_output_accuracy: 0.5106 - val_sub_category_output_loss: 2.1666\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HARIHARAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "models = {}\n",
    "for model_type in ['SimpleRNN', 'LSTM', 'GRU', 'Bi-LSTM']:\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    model = build_multi_output_model(model_type)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {'category_output': y_category_train, 'sub_category_output': y_sub_category_train},\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.3,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_category_pred, y_sub_category_pred = model.predict(X_test)\n",
    "    y_category_pred_classes = np.argmax(y_category_pred, axis=1)\n",
    "    y_sub_category_pred_classes = np.argmax(y_sub_category_pred, axis=1)\n",
    "    y_category_test_classes = np.argmax(y_category_test, axis=1)\n",
    "    y_sub_category_test_classes = np.argmax(y_sub_category_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    category_accuracy = accuracy_score(y_category_test_classes, y_category_pred_classes)\n",
    "    sub_category_accuracy = accuracy_score(y_sub_category_test_classes, y_sub_category_pred_classes)\n",
    "    category_precision = precision_score(y_category_test_classes, y_category_pred_classes, average='weighted')\n",
    "    sub_category_precision = precision_score(y_sub_category_test_classes, y_sub_category_pred_classes, average='weighted')\n",
    "    category_recall = recall_score(y_category_test_classes, y_category_pred_classes, average='weighted')\n",
    "    sub_category_recall = recall_score(y_sub_category_test_classes, y_sub_category_pred_classes, average='weighted')\n",
    "    \n",
    "    # Log results\n",
    "    results.append({\n",
    "        'Model': model_type,\n",
    "        'Category_Accuracy': category_accuracy,\n",
    "        'Sub_Category_Accuracy': sub_category_accuracy,\n",
    "        'Category_Precision': category_precision,\n",
    "        'Sub_Category_Precision': sub_category_precision,\n",
    "        'Category_Recall': category_recall,\n",
    "        'Sub_Category_Recall': sub_category_recall\n",
    "    })\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f\"{model_type}_multi_output_model.h5\")\n",
    "    models[model_type] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and label encoders\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('category_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(category_encoder, f)\n",
    "\n",
    "with open('sub_category_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(sub_category_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model  Category_Accuracy  Sub_Category_Accuracy  Category_Precision  \\\n",
      "3    Bi-LSTM           0.721010               0.512909            0.711285   \n",
      "1       LSTM           0.717528               0.508626            0.707772   \n",
      "2        GRU           0.713405               0.489413            0.702869   \n",
      "0  SimpleRNN           0.705840               0.475924            0.669412   \n",
      "\n",
      "   Sub_Category_Precision  Category_Recall  Sub_Category_Recall  \n",
      "3                0.494072         0.721010             0.512909  \n",
      "1                0.491373         0.717528             0.508626  \n",
      "2                0.475337         0.713405             0.489413  \n",
      "0                0.435268         0.705840             0.475924  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results).sort_values(by=['Category_Accuracy', 'Sub_Category_Accuracy'], ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
