{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/mnt/data/new.csv')\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data['processed_info'] = data['processed_info'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Hyperparameters\n",
    "max_vocab = 10000\n",
    "embedding_dim = 64\n",
    "max_length = 100\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(train_data['processed_info'])\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['processed_info']), maxlen=max_length)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['processed_info']), maxlen=max_length)\n",
    "\n",
    "# Encode target labels\n",
    "category_labels = train_data['category'].factorize()[0]\n",
    "subcategory_labels = train_data['sub_category'].factorize()[0]\n",
    "\n",
    "# LSTM Model for predicting both category and subcategory\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding = Embedding(max_vocab, embedding_dim)(input_layer)\n",
    "lstm = LSTM(128, return_sequences=True)(embedding)\n",
    "lstm_output = LSTM(64)(lstm)\n",
    "\n",
    "# Separate outputs\n",
    "category_output = Dense(len(set(category_labels)), activation='softmax', name=\"category_output\")(lstm_output)\n",
    "subcategory_output = Dense(len(set(subcategory_labels)), activation='softmax', name=\"subcategory_output\")(lstm_output)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[category_output, subcategory_output])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, [category_labels, subcategory_labels], epochs=5, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Crime Category & Subcategory Classification\")\n",
    "user_input = st.text_input(\"Enter crime description:\")\n",
    "if user_input:\n",
    "    preprocessed_input = preprocess_text(user_input)\n",
    "    tokenized_input = pad_sequences(tokenizer.texts_to_sequences([preprocessed_input]), maxlen=max_length)\n",
    "    category_pred, subcategory_pred = model.predict(tokenized_input)\n",
    "    \n",
    "    category = category_labels.inverse_transform([category_pred.argmax()])\n",
    "    subcategory = subcategory_labels.inverse_transform([subcategory_pred.argmax()])\n",
    "    \n",
    "    st.write(f\"Predicted Category: {category[0]}\")\n",
    "    st.write(f\"Predicted Subcategory: {subcategory[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to evaluate model\n",
    "def evaluate_model(model):\n",
    "    results = model.evaluate(X_test, [test_data['category'], test_data['sub_category']])\n",
    "    print(f\"Category Accuracy: {results[1]}, Subcategory Accuracy: {results[3]}\")\n",
    "\n",
    "# Example evaluation\n",
    "evaluate_model(model)  # Use different model names to evaluate each one\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
